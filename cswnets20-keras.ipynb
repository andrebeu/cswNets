{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import itertools\n",
    "\n",
    "from CSW import CSWTask\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as kr\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(env_size,env_dim,tsteps,hid_dim):\n",
    "  \"\"\" create a model \n",
    "  \"\"\"\n",
    "  # input layer\n",
    "  inputs = kr.Input(shape=(tsteps,env_dim), dtype=\"float32\")\n",
    "  # dense in\n",
    "  x = layers.Dense(hid_dim, activation=\"linear\")(inputs)\n",
    "  # rnn\n",
    "  x,h,c = kr.layers.LSTM(hid_dim, \n",
    "                         return_sequences=True,\n",
    "                         unroll=False,\n",
    "                         return_state=True,\n",
    "                         )(x,initial_state=None)\n",
    "  # dense out\n",
    "  outputs = layers.Dense(env_dim, activation=\"linear\")(x)\n",
    "  model = kr.Model(inputs, outputs)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(nnets=3,block_len0=40,learn_rate=0.05,hid_dim=35):\n",
    "  # net params\n",
    "  env_dim = 12\n",
    "  env_size = 12 \n",
    "  tsteps = 5\n",
    "  # task params\n",
    "  neps = 200\n",
    "\n",
    "  # task\n",
    "  taskL = [CSWTask(1),CSWTask(0)]\n",
    "  # embedding matrix\n",
    "  Emat = np.random.normal(0,1,[env_size,env_dim]).astype('float32')\n",
    "  # init  eval array\n",
    "  num_probes = 2 \n",
    "  acc = -np.ones([nnets,neps,num_probes]) \n",
    "\n",
    "  # loss and optimizer\n",
    "  loss_fn = kr.losses.MeanSquaredError()\n",
    "  optimizer = kr.optimizers.Adam(learning_rate=learn_rate)\n",
    "\n",
    "  for seed in range(nnets):\n",
    "    model = init_net(env_size,env_dim,tsteps,hid_dim)\n",
    "    task_int = 0\n",
    "    for ep in range(neps):\n",
    "      block_len = block_len0 \n",
    "      # controls test phase\n",
    "      if ep >= 160:\n",
    "        block_len = 1\n",
    "      # select graph\n",
    "      if ep%block_len==0:\n",
    "        task_int = (task_int+1)%2\n",
    "        task = taskL[task_int]\n",
    "        filler_id = 10+task_int \n",
    "      # sample path from graph\n",
    "      path = task.sample_path()\n",
    "      # format for neural network input\n",
    "      xtrain,ytrain = task.dataset_onestory_with_marker(path=path,filler_id=filler_id,depth=1,keras=True)\n",
    "      # embed \n",
    "      xtrain,ytrain = Emat[xtrain],Emat[ytrain]\n",
    "      # compute loss in tape\n",
    "      with tf.GradientTape() as tape:\n",
    "        yhat = model(xtrain,training=True)\n",
    "        loss_value = loss_fn(ytrain, yhat)\n",
    "      # eval \n",
    "      acc[seed,ep,0] = distance.cosine(yhat.numpy()[0,3],ytrain[0,3])\n",
    "      acc[seed,ep,1] = distance.cosine(yhat.numpy()[0,4],ytrain[0,4])\n",
    "      # update weights\n",
    "      grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "  return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation specific params\n",
    "nnets = 10\n",
    "learn_rate=0.05\n",
    "hid_dim=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_i = train_and_eval(nnets,1,learn_rate,hid_dim)\n",
    "acc_b = train_and_eval(nnets,40,learn_rate,hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axarr = plt.subplots(3,1,figsize=(12,10))\n",
    "# line plt\n",
    "ax = axarr[0]\n",
    "ax.set_title('interleaved')\n",
    "ax.plot(acc_i.mean(0))\n",
    "ax = axarr[1]\n",
    "ax.set_title('blocked')\n",
    "ax.plot(acc_b.mean(0))\n",
    "for ax in axarr[:2]:\n",
    "  ax.set_ylim(0,1.1)\n",
    "# bar plt\n",
    "ax = axarr[2]\n",
    "ax.set_title('early test phase')\n",
    "Mi = acc_i[:,-40:-35,:].mean()\n",
    "Si = acc_i[:,-40:-35,:].std()/np.sqrt(nnets)\n",
    "Mb = acc_b[:,-40:-35,:].mean()\n",
    "Sb = acc_b[:,-40:-35,:].std()/np.sqrt(nnets)\n",
    "ax.bar([1,2],[Mi,Mb],yerr=[Si,Sb],color='r')\n",
    "ax.set_xticks([1,2])\n",
    "ax.set_xticklabels(['interleaved','blocked'])\n",
    "# plt.savefig('matchSEM/keras/cosine_dist_yhat_ytarget_eucledian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
